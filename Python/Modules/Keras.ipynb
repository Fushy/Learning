{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "train_images: ndarray[uint8, tuple[int, int, int]]\n",
    "train_labels: ndarray[uint8, tuple[int, int, int]]\n",
    "test_images: ndarray[uint8, tuple[int, int, int]]\n",
    "test_labels: ndarray[uint8, tuple[int, int, int]]\n",
    "    \n",
    "training_set, test_set = mnist.load_data()\n",
    "train_images, train_labels = training_set\n",
    "test_images, test_labels = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The layers\n",
    "The core building block of neural networks is the layer, a data-processing module that you can think of as a filter for data.\n",
    "Some data goes in, and it comes out in a more useful form. Layers extract representations out of the data fed into them—hopefully, representations that are more meaningful for the problem at hand. Most of the deep learning consists of chaining together simple layers that will implement a form of progressive data distillation.\n",
    "A deep-learning model is like a sieve for data processing, made of a succession of increasingly refined data filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network consists of a sequence of two Dense layers, which are densely connected (called fully connected) neural layers. The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\n",
    "To make the network ready for training, we need to pick three more things, as part of the compilation step:\n",
    "+ A loss function—How the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.\n",
    "+ An optimizer—The mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "+ Metrics to monitor during training and testing—Here, we’ll only care about accuracy (the fraction of the images that were correctly classified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we’ll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval. Our training images were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
